{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as util_data\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models, datasets\n",
    "from tools import *\n",
    "from make_dataset import *\n",
    "from ST_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cls=xxxxxx\n",
    "bit=xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PENet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hash_bit,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 embed_dim=96,\n",
    "                 depths=(2, 2, 6, 2),\n",
    "                 num_heads=(3, 6, 12, 24),\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 patch_norm=True,\n",
    "                 use_checkpoint=False,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2**(self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_c=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layers = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if\n",
    "                (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layers)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.hash = nn.Linear(self.num_features, hash_bit)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, H, W = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "        for layer in self.layers:\n",
    "            x, H, W = layer(x, H, W)\n",
    "        x = self.norm(x)\n",
    "        x = self.avgpool(x.transpose(1, 2))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.hash(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b401c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(ImageFolder):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        target = np.eye(num_cls, dtype=np.int8)[np.array(target)]\n",
    "\n",
    "        return sample, target, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    image_datasets ={}\n",
    "    dataloaders = {}\n",
    "    for data_set in [\"train\", \"test\", \"valid\"]:\n",
    "        image_datasets[data_set] = MyDataset(os.path.join(config[\"data_dir\"],data_set),\n",
    "                                                 transform=data_transforms[data_set] \n",
    "                                                )\n",
    "        dataloaders[data_set] = util_data.DataLoader(image_datasets[data_set],\n",
    "                                                          batch_size=12,\n",
    "                                                          shuffle=True)\n",
    "    return dataloaders[\"train\"], dataloaders[\"test\"], dataloaders[\"valid\"], \\\n",
    "               len(image_datasets[\"train\"]), len(image_datasets[\"test\"]), len(image_datasets[\"valid\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    config = {\n",
    "        \"alpha\": 0.1,\n",
    "        \"optimizer\": {\"type\": optim.AdamW, \"optim_params\": {\"lr\": xxxx, \"weight_decay\": 5E-2}, \"lr_type\": \"step\"},\n",
    "        \"epoch\": 50,\n",
    "        \"test_map\": 1,\n",
    "        \"save_path\": \"xxxxxx\",\n",
    "        \"device\": torch.device(\"cuda:0\"),\n",
    "        \"topK\": -1,\n",
    "        \"n_class\":num_cls,\n",
    "        \"data_dir\" : \"xxxxxx\",\n",
    "        \" batch_size\": 12,\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7826d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSHLoss(torch.nn.Module):\n",
    "    def __init__(self, config, bit):\n",
    "        super(DSHLoss, self).__init__()\n",
    "        self.m = 2 * bit\n",
    "        self.U = torch.zeros(config[\"num_train\"], bit).float().to(config[\"device\"])\n",
    "        self.Y = torch.zeros(config[\"num_train\"], config[\"n_class\"]).float().to(config[\"device\"])\n",
    "\n",
    "    def forward(self, u, y, ind, config):\n",
    "        self.U[ind, :] = u.data\n",
    "        self.Y[ind, :] = y.float()\n",
    "\n",
    "        dist = (u.unsqueeze(1) - self.U.unsqueeze(0)).pow(2).sum(dim=2)\n",
    "        y = (y @ self.Y.t() == 0).float()\n",
    "\n",
    "        loss = (1 - y) / 2 * dist + y / 2 * (self.m - dist).clamp(min=0)\n",
    "        loss1 = loss.mean()\n",
    "        loss2 = config[\"alpha\"] * (1 - u.sign()).abs().mean()\n",
    "\n",
    "        return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.RandomRotation(0),\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])       \n",
    "]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eecf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(hash_bit,num_classes, **kwargs):\n",
    "    # trained ImageNet-1K\n",
    "    # https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth\n",
    "    model = PENet(in_chans=3,\n",
    "                            hash_bit=hash_bit,\n",
    "                            patch_size=4,\n",
    "                            window_size=7,\n",
    "                            embed_dim=96,\n",
    "                            depths=(2, 2, 6, 2),\n",
    "                            num_heads=(3, 6, 12, 24),\n",
    "                            num_classes=num_classes,\n",
    "                            **kwargs)\n",
    "    weights_dict = torch.load('./xxxxxxx.pth', map_location=device)['model']\n",
    "    for k in list(weights_dict.keys()):\n",
    "        del weights_dict[k]\n",
    "    model.load_state_dict(weights_dict, strict=False)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config[\"device\"]\n",
    "net = get_model(hash_bit=bit,num_classes=num_cls).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5c9b7",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = config[\"optimizer\"][\"type\"](net.parameters(), **(config[\"optimizer\"][\"optim_params\"]))\n",
    "train_loader,  dataset_loader,test_loader, num_train, num_dataset, num_test = get_data(config)\n",
    "config[\"num_train\"] = num_train\n",
    "criterion = DSHLoss(config, bit)\n",
    "Best_mAP = 0\n",
    "for epoch in range(config[\"epoch\"]):\n",
    "    current_time = time.strftime('%H:%M:%S', time.localtime(time.time()))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for image, label, ind in train_loader:\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        u = net(image)\n",
    "\n",
    "        loss = criterion(u, label.float(), ind, config)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    print(\"\\b\\b\\b\\b\\b\\b\\b loss:%.3f\" % (train_loss))\n",
    "\n",
    "    if (epoch + 1) % config[\"test_map\"] == 0:\n",
    "        tst_binary, tst_label = compute_result(test_loader, net, device=device)\n",
    "        trn_binary, trn_label = compute_result(dataset_loader, net, device=device)\n",
    "        mAP = CalcTopMap(trn_binary.numpy(), tst_binary.numpy(), trn_label.numpy(), tst_label.numpy(),\n",
    "                             config[\"topK\"])\n",
    "\n",
    "        if mAP > Best_mAP:\n",
    "            Best_mAP = mAP\n",
    "            if \"save_path\" in config:\n",
    "                if not os.path.exists(config[\"save_path\"]):\n",
    "                    os.makedirs(config[\"save_path\"])\n",
    "                print(\"save in \", config[\"save_path\"])\n",
    "                np.save(os.path.join(config[\"save_path\"], \"xxxxx\"+str(bit) + str(mAP) + \"-\" + \"trn_binary.npy\"),\n",
    "                         trn_binary.numpy())\n",
    "                torch.save(net.state_dict(),\n",
    "                            os.path.join(config[\"save_path\"], \"xxxxx\"+str(bit) + \"-\" + str(mAP) + \"-model.pt\"))\n",
    "        print(\" epoch:%d, bit:%d, MAP:%.3f, Best MAP: %.3f\" % (\n",
    "                 epoch + 1, bit, mAP, Best_mAP))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444a3e2",
   "metadata": {},
   "source": [
    "## img_to_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset_test = MyDataset('./path',\n",
    "                                                 transform=data_transforms[\"train\"] \n",
    "                                                )\n",
    "test_dataloaders = util_data.DataLoader(image_dataset_test,\n",
    "                                                          batch_size=12,\n",
    "                                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de91067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_result(dataloader, net, device):\n",
    "    bs, clses = [], []\n",
    "    net.eval()\n",
    "    for img, cls, _ in tqdm(dataloader):\n",
    "        clses.append(cls)\n",
    "        bs.append((net(img.to(device))).data.cpu())\n",
    "    hash_codes=torch.cat(bs).sign().numpy()\n",
    "    hash_codes[hash_codes<0]=0\n",
    "    return hash_codes, torch.cat(clses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_codes, label = compute_result(test_dataloaders, net, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9d717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
